{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Recognition & Context Management Integration\n",
    "\n",
    "*This notebook integrates existing text cleaning and processing pipeline with intent recognition using Word2Vec-style embeddings (from the pre-trained GloVe model) and context management. We use an expanded dataset of common intents to train a classifier. The predicted intent and updated context for each input are printed without generating a response.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import yaml\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy.linalg import norm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load pre-trained GloVe model (using gensim)\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "pretrained_model = api.load(\"glove-wiki-gigaword-100\")  # 100-dimensional embeddings\n",
    "embed_dim = pretrained_model.vector_size\n",
    "\n",
    "# Load a pre-trained word embedding model.\n",
    "# pretrained_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Load the Google News Word2Vec model (300-dimensional)\n",
    "# pretrained_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Load the GloVe Twitter model (e.g., 25-dimensional)\n",
    "# pretrained_model = api.load(\"glove-twitter-25\")\n",
    "\n",
    "# Load the FastText model (300-dimensional with subword information)\n",
    "# pretrained_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# Load the ConceptNet Numberbatch model (300-dimensional)\n",
    "# pretrained_model = api.load(\"conceptnet-numberbatch-17-06-300\")\n",
    "\n",
    "english_stop_words = list(set(stopwords.words(\"english\")))\n",
    "# print(english_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(prompt):\n",
    "    return prompt.lower()\n",
    "\n",
    "def delete_stopwords(prompt):\n",
    "    return \" \".join([word for word in prompt.split() if word not in english_stop_words])\n",
    "\n",
    "def text_cleaning(prompt):\n",
    "    ignore_character = list(string.punctuation)\n",
    "    pattern = f\"[{re.escape(''.join(ignore_character))}]\"\n",
    "    cleaned_prompt = re.sub(pattern, \" \", prompt)\n",
    "    cleaned_prompt = re.sub(r\"\\b[a-z]\\b\", \"\", cleaned_prompt)\n",
    "    cleaned_prompt = re.sub(r\"\\s+\", \" \", cleaned_prompt)\n",
    "    return cleaned_prompt.strip()\n",
    "\n",
    "def tokenization(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def lemmatization(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def nettoyage_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Process a list of documents: lowercasing, stopword deletion,\n",
    "    cleaning, tokenization, and lemmatization.\n",
    "    \"\"\"\n",
    "    cleaned_conversations = [\n",
    "        lemmatization(tokenization(text_cleaning(delete_stopwords(to_lowercase(doc)))))\n",
    "        for doc in corpus\n",
    "    ]\n",
    "    return cleaned_conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(model, sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    valid_tokens = [token for token in tokens if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    embeddings = [model[token] for token in valid_tokens]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanded Intent Dataset and Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26688     0.39632     0.6169     ...  0.35842    -0.48464\n",
      "   0.30728   ]\n",
      " [ 0.1444      0.23979     0.96693    ... -0.72424    -0.22632\n",
      "  -0.030972  ]\n",
      " [ 0.1467225   0.561025    0.72091496 ... -0.324925    0.11294499\n",
      "   0.42388   ]\n",
      " ...\n",
      " [-0.00800675  0.48382002  0.43498504 ... -0.5387625   0.057307\n",
      "   0.52885497]\n",
      " [-0.03922867 -0.06117199  0.45376667 ... -0.16548167  0.17534967\n",
      "   0.13063633]\n",
      " [-0.23837     0.278805    0.462135   ... -0.32915503 -0.0046095\n",
      "   0.41613   ]]\n"
     ]
    }
   ],
   "source": [
    "intent_phrases = {\n",
    "    \"greeting\": [\n",
    "        \"Hello\", \"Hi\", \"Hey there\", \"Good morning\", \"Good afternoon\", \"Good evening\",\n",
    "        \"What's up\", \"Greetings\", \"Howdy\", \"Hi, how are you?\", \"Hey\", \"Hello there\",\n",
    "        \"Hey, what's going on?\", \"Yo\", \"Hiya\", \"Hello, nice to see you!\", \"Hey buddy\",\n",
    "        \"Good to see you\", \"Hi, hope you're well\", \"Hello, how do you do?\"\n",
    "    ],\n",
    "    \"goodbye\": [\n",
    "        \"Goodbye\", \"Bye\", \"See you later\", \"Talk to you soon\", \"Farewell\", \"Take care\",\n",
    "        \"Catch you later\", \"See ya\", \"Bye bye\", \"Adios\", \"Later\", \"So long\", \"Good night\",\n",
    "        \"I'm off\", \"Peace out\", \"Ciao\", \"Until next time\", \"Farewell for now\",\n",
    "        \"See you around\", \"Later alligator\"\n",
    "    ],\n",
    "    \"get_time\": [\n",
    "        \"What time is it?\", \"Tell me the current time\", \"Could you give me the time?\",\n",
    "        \"I need to know the time\", \"Time please\", \"Do you know what time it is?\",\n",
    "        \"Can you tell me the time?\", \"What's the time now?\", \"Please share the time\",\n",
    "        \"Current time?\", \"Time update\", \"What's the clock saying?\", \"Show me the time\",\n",
    "        \"Time check\", \"What's the time, please?\", \"May I know the time?\",\n",
    "        \"Could you update me with the time?\", \"Time now?\", \"Let me know the time\", \"Time?\"\n",
    "    ],\n",
    "    \"get_weather\": [\n",
    "        \"What's the weather like today?\", \"Tell me the weather forecast\", \"How is the weather?\",\n",
    "        \"Is it going to rain?\", \"Weather update please\", \"What's the temperature outside?\",\n",
    "        \"Do I need an umbrella today?\", \"Weather report\", \"Current weather conditions?\",\n",
    "        \"How's the weather outside?\", \"Forecast for today?\", \"Is it sunny or rainy?\",\n",
    "        \"Weather status\", \"What's the climate like today?\", \"Do I need a jacket today?\",\n",
    "        \"How's the weather looking?\", \"Any rain expected today?\", \"Weather check\",\n",
    "        \"Let me know today's weather\", \"Weather update\"\n",
    "    ],\n",
    "    \"thanks\": [\n",
    "        \"Thank you\", \"Thanks a lot\", \"Much appreciated\", \"Thanks\", \"Thank you very much\",\n",
    "        \"I appreciate it\", \"Thanks a million\", \"Thank you so much\", \"Cheers\", \"Thanks a bunch\",\n",
    "        \"Many thanks\", \"I'm grateful\", \"Thank you kindly\", \"I owe you one\", \"Appreciate it\",\n",
    "        \"Thanks for everything\", \"Thanks, that was helpful\", \"Thank you, really appreciate it\",\n",
    "        \"Thanks a ton\", \"Sincere thanks\"\n",
    "    ],\n",
    "    \"apology\": [\n",
    "        \"I'm sorry\", \"My apologies\", \"Sorry for that\", \"I apologize\", \"Please forgive me\",\n",
    "        \"Sorry about that\", \"My bad\", \"I didn't mean that\", \"I am really sorry\", \"Apologies\",\n",
    "        \"I regret that\", \"So sorry\", \"Excuse me\", \"Pardon me\", \"I beg your pardon\",\n",
    "        \"I sincerely apologize\", \"Forgive me, please\", \"I apologize for any inconvenience\",\n",
    "        \"I'm truly sorry\", \"Sorry, my mistake\"\n",
    "    ],\n",
    "    \"unknown\": [\n",
    "        \"I don't know\", \"Can you repeat that?\", \"What do you mean?\", \"I don't understand\",\n",
    "        \"Could you say that again?\", \"Not sure what you mean\", \"I'm confused\", \"What?\",\n",
    "        \"Huh?\", \"I have no idea\", \"Could you clarify?\", \"I didn't catch that\",\n",
    "        \"Sorry, what did you say?\", \"I am not sure I follow\", \"Please explain\",\n",
    "        \"I don't follow\", \"Could you rephrase that?\", \"I don't comprehend\", \"Unclear to me\", \"Not sure\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for intent, phrases in intent_phrases.items():\n",
    "    for phrase in phrases:\n",
    "        texts.append(phrase)\n",
    "        labels.append(intent)\n",
    "\n",
    "X = np.array([get_sentence_embedding(pretrained_model, text) for text in texts])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model[Hello] = [ 0.1444    0.23979   0.96693   0.31629  -0.36064  -0.87674   0.098512\n",
      "  0.31078   0.47929   0.27175   0.30005  -0.23732  -0.31517   0.17925\n",
      "  0.61773   0.59821   0.49489   0.3423   -0.078034  0.60212   0.18683\n",
      "  0.5207   -0.12331   0.48313  -0.24117   0.59696   0.61078  -0.84414\n",
      "  0.27661   0.068767 -1.1388    0.089544  0.89842   0.53788   0.10841\n",
      " -0.10038   0.12921   0.11476  -0.474    -0.8049    0.96     -0.36602\n",
      " -0.43019  -0.39808  -0.096782 -0.71184  -0.31494   0.82346   0.42179\n",
      " -0.69205  -1.4864    0.29498  -0.30875  -0.49995  -0.4649   -0.44524\n",
      "  0.8106    1.4757    0.53782  -0.28271  -0.045796  0.14454  -0.74485\n",
      "  0.35495  -0.40961   0.35779   0.40061   0.37339   0.72163   0.40813\n",
      "  0.26155  -0.14239  -0.020514 -1.1106   -0.4767    0.37832   0.89612\n",
      " -0.17323  -0.50137   0.22991   1.5324   -0.82032  -0.10096   0.45202\n",
      " -0.88639   0.089056 -0.19347  -0.42253   0.022429  0.29444   0.020747\n",
      "  0.48935   0.35991   0.092758 -0.22428   0.60038  -0.3185   -0.72424\n",
      " -0.22632  -0.030972]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"pretrained_model[Hello] = {pretrained_model[\"hi\"]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_bot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
