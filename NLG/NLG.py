import os
import re
import string
import yaml
import numpy as np
import nltk
import gensim.downloader as api
import torch
import torch.nn as nn
import torch.optim as optim
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from numpy.linalg import norm
from sklearn.linear_model import LogisticRegression

# -------------------- Setup and Downloads --------------------
nltk.download("stopwords")
nltk.download("punkt")
nltk.download("wordnet")

# Load pre-trained GloVe model for retrieval-based generation (100-dimensional)
print("Loading GloVe embeddings...")
pretrained_model = api.load("glove-wiki-gigaword-100")
embed_dim = pretrained_model.vector_size

# -------------------- Text Pre-Processing Functions --------------------
english_stop_words = list(set(stopwords.words("english")))


def to_lowercase(prompt):
    return prompt.lower()


def delete_stopwords(prompt):
    return " ".join([word for word in prompt.split() if word not in english_stop_words])


def text_cleaning(prompt):
    pattern = f"[{re.escape(''.join(string.punctuation))}]"
    cleaned_prompt = re.sub(pattern, " ", prompt)
    cleaned_prompt = re.sub(r"\b[a-z]\b", "", cleaned_prompt)
    cleaned_prompt = re.sub(r"\s+", " ", cleaned_prompt)
    return cleaned_prompt.strip()


def tokenization(sentence):
    return word_tokenize(sentence)


def lemmatization(tokens):
    lemmatizer = nltk.stem.WordNetLemmatizer()
    return [lemmatizer.lemmatize(token) for token in tokens]


def nettoyage_corpus(corpus):
    """
    Process a list of documents: lowercasing, stopword deletion,
    cleaning, tokenization, and lemmatization.
    """
    cleaned_conversations = [
        lemmatization(tokenization(text_cleaning(delete_stopwords(to_lowercase(doc)))))
        for doc in corpus
    ]
    return cleaned_conversations


# -------------------- Sentence Embedding --------------------
def get_sentence_embedding(model, sentence):
    tokens = word_tokenize(sentence.lower())
    valid_tokens = [token for token in tokens if token in model]
    if not valid_tokens:
        return np.zeros(model.vector_size)
    embeddings = [model[token] for token in valid_tokens]
    return np.mean(embeddings, axis=0)


# -------------------- Retrieval-Based Response Generation --------------------
# Enhanced candidate responses for intents, including a new "eilco" intent.
candidate_responses = {
    "greeting": [
        "Hello! How can I help you today?",
        "Hi there, welcome to our chatbot!",
        "Hey! What would you like to discuss today?",
        "Hello! Feel free to ask anything, including details about EiLCO.",
    ],
    "goodbye": [
        "Goodbye! Have a great day.",
        "Bye! Take care.",
        "See you later, and remember, EiLCO is always here to support your journey.",
    ],
    "get_time": [
        "The current time is 3:45 PM.",  # placeholder text; can be replaced by a live function
        "It's 3:45 in the afternoon right now.",
        "Right now, it's 3:45 PM.",
    ],
    "get_weather": [
        "It's sunny and 25°C outside.",
        "Currently, it's sunny with a temperature of 25°C.",
        "The weather is clear and warm at 25°C.",
    ],
    "thanks": [
        "You're welcome!",
        "No problem, happy to help!",
        "Anytime! If you need more info on EiLCO, just ask.",
    ],
    "apology": [
        "No worries, it's okay.",
        "Apology accepted.",
        "Don't worry about it. Let me know if you need any clarification.",
    ],
    "unknown": [
        "I'm not sure I understand. Could you please clarify?",
        "Sorry, I didn't catch that. Can you rephrase?",
        "I don't understand. Can you explain a bit more?",
    ],
    "eilco": [
        "EiLCO stands for École d'Ingénieurs du Littoral Côte d'Opale. We offer innovative engineering programs and foster strong industry connections.",
        "At EiLCO, our mission is to empower students with hands-on skills in engineering and technology tailored for coastal industries.",
        "EiLCO is a leading school for coastal engineering, where future engineers learn through real-world projects, innovative research, and personalized mentoring.",
        "If you’re interested in the engineering sector with a focus on coastal innovations, EiLCO has a range of programs and opportunities just for you.",
    ],
}


def cosine_similarity(vec1, vec2):
    if norm(vec1) == 0 or norm(vec2) == 0:
        return 0.0
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))


# Enhanced training data for intent classification.
intent_phrases = {
    "greeting": [
        "Hello",
        "Hi",
        "Hey there",
        "Good morning",
        "Good afternoon",
        "Good evening",
        "What's up",
        "Greetings",
        "Howdy",
        "Hi, how are you?",
        "Hey",
        "Hello there",
    ],
    "goodbye": [
        "Goodbye",
        "Bye",
        "See you later",
        "Talk to you soon",
        "Farewell",
        "Take care",
        "Catch you later",
        "See ya",
        "Bye bye",
        "Adios",
        "Later",
        "So long",
    ],
    "get_time": [
        "What time is it?",
        "Tell me the current time",
        "Could you give me the time?",
        "I need to know the time",
        "Time please",
        "Do you know what time it is?",
    ],
    "get_weather": [
        "What's the weather like today?",
        "Tell me the weather forecast",
        "How is the weather?",
        "Is it going to rain?",
        "Weather update please",
        "What's the temperature outside?",
    ],
    "thanks": [
        "Thank you",
        "Thanks a lot",
        "Much appreciated",
        "Thanks",
        "Thank you very much",
        "I appreciate it",
        "Thanks a million",
    ],
    "apology": [
        "I'm sorry",
        "My apologies",
        "Sorry for that",
        "I apologize",
        "Please forgive me",
        "Sorry about that",
    ],
    "unknown": [
        "I don't know",
        "Can you repeat that?",
        "What do you mean?",
        "I don't understand",
        "Could you say that again?",
        "I'm confused",
    ],
    "eilco": [
        "Tell me about EiLCO",
        "I want to know about EiLCO",
        "What is EiLCO?",
        "EiLCO programs",
        "Engineering at EiLCO",
        "Coastal engineering school",
        "EiLCO details",
        "Information about EiLCO",
    ],
}

# Combine data for training intent classifier.
texts = []
labels = []
for intent, phrases in intent_phrases.items():
    for phrase in phrases:
        texts.append(phrase)
        labels.append(intent)

X = np.array([get_sentence_embedding(pretrained_model, text) for text in texts])
y = np.array(labels)

clf = LogisticRegression(max_iter=1000)
clf.fit(X, y)


def ml_intent(sentence):
    embedding = get_sentence_embedding(pretrained_model, sentence).reshape(1, -1)
    return clf.predict(embedding)[0]


def select_response(user_input, predicted_intent):
    """
    Given a user input and predicted intent, choose the best candidate response by measuring
    cosine similarity between embeddings.
    """
    input_embedding = get_sentence_embedding(pretrained_model, user_input)
    responses = candidate_responses.get(
        predicted_intent, candidate_responses["unknown"]
    )
    best_response = None
    best_similarity = -1
    for response in responses:
        resp_embedding = get_sentence_embedding(pretrained_model, response)
        similarity = cosine_similarity(input_embedding, resp_embedding)
        if similarity > best_similarity:
            best_similarity = similarity
            best_response = response
    return best_response


def retrieval_based_response(user_input):
    """
    Generate a response using the retrieval-based approach.
    """
    cleaned_corpus = nettoyage_corpus([user_input])
    cleaned_text = " ".join(cleaned_corpus[0])
    predicted_intent = ml_intent(cleaned_text)
    response = select_response(cleaned_text, predicted_intent)
    return response


# -------------------- Seq2Seq Model --------------------
# --- Data Preparation ---
def clean_text(text, remove_stopwords=False):
    if isinstance(text, list):
        return [clean_text(t, remove_stopwords) for t in text]
    text = text.lower()
    text = re.sub(r"[{}]".format(re.escape(string.punctuation)), " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    tokens = word_tokenize(text)
    lemmatizer = nltk.stem.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    if remove_stopwords:
        tokens = [token for token in tokens if token not in english_stop_words]
    return tokens


# Enhanced conversational dataset with many new pairs including EiLCO-related conversations.
pairs = [
    ("hello", "hi there"),
    ("hi", "hello, how can I help you?"),
    ("good morning", "good morning! how can I assist you today?"),
    ("hey", "hey there, what can I do for you?"),
    ("how are you", "i am fine, thanks for asking."),
    ("what's up", "not much, how about you?"),
    ("what is your name", "i am a chatbot, here to assist you."),
    ("who are you", "i am your virtual assistant, ready to help."),
    ("goodbye", "see you later, take care!"),
    ("bye", "goodbye, have a nice day!"),
    ("thanks", "you're welcome!"),
    ("thank you", "no problem, happy to help!"),
    ("i need help", "sure, what do you need help with?"),
    ("can you help me", "of course, how can i assist you?"),
    ("what time is it", "the current time is 3:45 pm."),
    ("what's the weather", "it's sunny and 25°C outside."),
    (
        "i am having a technical issue",
        "i'm sorry to hear that, can you describe the problem?",
    ),
    (
        "i want to check my order status",
        "please provide your order number so i can check.",
    ),
    (
        "i would like a refund",
        "i'm sorry for the inconvenience. please share your order number for processing.",
    ),
    ("i don't understand", "could you please rephrase that?"),
    ("can you repeat that", "sure, let me repeat that for you."),
    ("what is your purpose", "i am here to assist you with any questions or tasks."),
    (
        "tell me a joke",
        "why did the scarecrow win an award? because he was outstanding in his field!",
    ),
    (
        "what can you do",
        "i can help answer your questions, provide information, and assist with tasks.",
    ),
    (
        "how can i reset my password",
        "you can reset your password by clicking on 'forgot password' on the login page.",
    ),
    ("i am bored", "maybe try a new hobby, or i can share a fun fact with you."),
    ("tell me a fun fact", "did you know that honey never spoils?"),
    ("i need some advice", "what kind of advice are you looking for?"),
    ("what is the meaning of life", "that's a deep question! some say it's 42."),
    (
        "do you know any good restaurants",
        "i can recommend some if you tell me your location.",
    ),
    (
        "i want to book a flight",
        "sure, i can help with that. can you provide your travel dates?",
    ),
    ("can i talk to a human", "i can connect you with a human agent, please hold on."),
    # New conversational pairs about EiLCO:
    (
        "what is eilco",
        "EiLCO stands for École d'Ingénieurs du Littoral Côte d'Opale, a school focused on innovative coastal engineering.",
    ),
    (
        "tell me about eilco",
        "EiLCO offers hands-on engineering programs with a focus on coastal technologies and sustainable development.",
    ),
    (
        "what programs does eilco offer",
        "EiLCO offers various engineering programs including coastal engineering, environmental technology, and industrial innovation.",
    ),
    (
        "where is eilco located",
        "EiLCO is located along the beautiful Côte d'Opale, providing a unique setting for learning and research.",
    ),
    (
        "why should i study at eilco",
        "Studying at EiLCO means you receive practical training in coastal engineering, industry partnerships, and personalized mentoring.",
    ),
    (
        "how is the campus at eilco",
        "The campus at EiLCO is modern and innovative, designed to foster collaboration and hands-on learning.",
    ),
    (
        "can you give me more details about eilco",
        "Certainly! EiLCO combines rigorous academic training with practical experience in coastal and marine engineering projects.",
    ),
    (
        "what makes eilco unique",
        "EiLCO stands out for its unique focus on coastal and maritime challenges, offering specialized courses and hands-on projects.",
    ),
    (
        "i want to know about coastal engineering",
        "Coastal engineering at EiLCO focuses on sustainable development, innovative design, and environmental responsibility.",
    ),
]

# Preprocess input and target texts for Seq2Seq training.
input_texts = [pair[0] for pair in pairs]
target_texts = [pair[1] for pair in pairs]

# Add special tokens for decoder output.
START_TOKEN = "<start>"
END_TOKEN = "<end>"
target_tokens = [[START_TOKEN] + clean_text([t])[0] + [END_TOKEN] for t in target_texts]

input_tokens = clean_text(input_texts, remove_stopwords=False)


# Build vocabulary from tokens.
def build_vocab(tokenized_texts, min_freq=1):
    freq = {}
    for tokens in tokenized_texts:
        for token in tokens:
            freq[token] = freq.get(token, 0) + 1
    vocab = {token for token, count in freq.items() if count >= min_freq}
    vocab = sorted(list(vocab))
    # Reserve index 0 for <pad> and index 1 for <unk>
    word2idx = {word: idx + 2 for idx, word in enumerate(vocab)}
    word2idx["<pad>"] = 0
    word2idx["<unk>"] = 1
    idx2word = {idx: word for word, idx in word2idx.items()}
    return word2idx, idx2word


all_tokens = input_tokens + target_tokens
word2idx, idx2word = build_vocab(all_tokens)
vocab_size = len(word2idx)
print("Vocabulary size:", vocab_size)

# Create embedding matrix using GloVe.
embedding_matrix = np.zeros((vocab_size, embed_dim))
for word, idx in word2idx.items():
    if word in pretrained_model:
        embedding_matrix[idx] = pretrained_model[word]
    else:
        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embed_dim,))
embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)


# Convert tokens to indices and pad sequences.
def tokens_to_indices(tokens, word2idx):
    return [word2idx.get(token, word2idx["<unk>"]) for token in tokens]


def pad_sequence(seq, max_len):
    return seq + [word2idx["<pad>"]] * (max_len - len(seq))


encoder_inputs = [tokens_to_indices(tokens, word2idx) for tokens in input_tokens]
decoder_inputs = [tokens_to_indices(tokens, word2idx) for tokens in target_tokens]

encoder_max_len = max(len(seq) for seq in encoder_inputs)
decoder_max_len = max(len(seq) for seq in decoder_inputs)

encoder_inputs = [pad_sequence(seq, encoder_max_len) for seq in encoder_inputs]
decoder_inputs = [pad_sequence(seq, decoder_max_len) for seq in decoder_inputs]

encoder_inputs = torch.tensor(encoder_inputs, dtype=torch.long)
decoder_inputs = torch.tensor(decoder_inputs, dtype=torch.long)


# -------------------- Seq2Seq Model Definition --------------------
class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, embedding_matrix):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.embedding.weight = nn.Parameter(embedding_matrix)
        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)

    def forward(self, x):
        embedded = self.embedding(x)
        outputs, (hidden, cell) = self.lstm(embedded)
        return hidden, cell


class Decoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, embedding_matrix):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.embedding.weight = nn.Parameter(embedding_matrix)
        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, input_token, hidden, cell):
        input_token = input_token.unsqueeze(1)
        embedded = self.embedding(input_token)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc(output.squeeze(1))
        return prediction, hidden, cell


hidden_size = 256
num_epochs = 300
learning_rate = 0.001
batch_size = encoder_inputs.size(
    0
)  # using full dataset as a single batch for simplicity

encoder = Encoder(vocab_size, embed_dim, hidden_size, embedding_matrix)
decoder = Decoder(vocab_size, embed_dim, hidden_size, embedding_matrix)

criterion = nn.CrossEntropyLoss(ignore_index=word2idx["<pad>"])
optimizer = optim.Adam(
    list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate
)

# -------------------- Model Training / Loading --------------------
encoder_weights_file = "encoder_weights.pth"
decoder_weights_file = "decoder_weights.pth"


def train_seq2seq():
    encoder.train()
    decoder.train()
    for epoch in range(1, num_epochs + 1):
        optimizer.zero_grad()
        hidden, cell = encoder(encoder_inputs)
        decoder_input = decoder_inputs[
            :, 0
        ]  # starting with start tokens for each sample
        loss = 0
        for t in range(1, decoder_max_len):
            output, hidden, cell = decoder(decoder_input, hidden, cell)
            loss += criterion(output, decoder_inputs[:, t])
            decoder_input = decoder_inputs[:, t]  # Teacher forcing

        loss.backward()
        optimizer.step()

        if epoch % 50 == 0:
            print(
                f"Epoch [{epoch}/{num_epochs}], Loss: {loss.item() / (decoder_max_len - 1):.4f}"
            )
    torch.save(encoder.state_dict(), encoder_weights_file)
    torch.save(decoder.state_dict(), decoder_weights_file)
    print("Seq2Seq model trained and weights saved.")


if os.path.exists(encoder_weights_file) and os.path.exists(decoder_weights_file):
    print("Loading saved Seq2Seq model weights...")
    encoder.load_state_dict(torch.load(encoder_weights_file))
    decoder.load_state_dict(torch.load(decoder_weights_file))
else:
    print("Saved weights not found. Training Seq2Seq model...")
    train_seq2seq()


def generate_response(input_sentence, max_len=20):
    """
    Generate a response using the Seq2Seq model.
    """
    encoder.eval()
    decoder.eval()
    tokens = clean_text([input_sentence])[0]
    indices = tokens_to_indices(tokens, word2idx)
    indices = pad_sequence(indices, encoder_max_len)
    input_tensor = torch.tensor([indices], dtype=torch.long)

    with torch.no_grad():
        hidden, cell = encoder(input_tensor)
        decoder_input = torch.tensor([word2idx[START_TOKEN]], dtype=torch.long)
        output_sentence = []
        for _ in range(max_len):
            output, hidden, cell = decoder(decoder_input, hidden, cell)
            predicted_idx = output.argmax(1).item()
            if predicted_idx == word2idx.get(END_TOKEN, None):
                break
            output_sentence.append(idx2word.get(predicted_idx, "<unk>"))
            decoder_input = torch.tensor([predicted_idx], dtype=torch.long)
    return " ".join(output_sentence)


def seq2seq_response(user_input):
    """
    Wrapper function for Seq2Seq model response generation.
    """
    return generate_response(user_input)


# -------------------- Additional Utilities --------------------
class ContextManager:
    def __init__(self):
        self.user_context = {}

    def update_context(self, user_id, intent):
        self.user_context[user_id] = intent

    def get_context(self, user_id):
        return self.user_context.get(user_id, None)


# -------------------- Wrapper Functions for Integration --------------------
def retrieval_based_response_wrapper(user_input):
    """
    Wrapper function to generate response using retrieval-based method.
    """
    return retrieval_based_response(user_input)


def seq2seq_response_wrapper(user_input):
    """
    Wrapper function to generate response using Seq2Seq model.
    """
    return seq2seq_response(user_input)


# -------------------- Example Testing (Optional) --------------------
if __name__ == "__main__":
    context_manager = ContextManager()
    user_id = "user123"
    test_sentences = [
        "hello",
        "What's up?",
        "good morning",
        "how are you today?",
        "what is your name?",
        "can you help me with my order?",
        "i need a refund",
        "i don't understand",
        "tell me a joke",
        "what's the weather like?",
        "i am having a technical issue",
        "can i talk to a human?",
        "what is eilco",
        "tell me about eilco",
        "why should i study at eilco",
    ]

    print("\n--- Retrieval-Based Responses ---")
    for sent in test_sentences:
        response = retrieval_based_response(sent)
        print(f"User: {sent}\nBot: {response}\n")

    print("\n--- Seq2Seq Generated Responses ---")
    for sent in test_sentences:
        response = generate_response(sent)
        print(f"User: {sent}\nBot: {response}\n")
