{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Recognition & Context Management Integration\n",
    "\n",
    "*This notebook integrates existing text cleaning and processing pipeline with intent recognition using Word2Vec-style embeddings (from the pre-trained GloVe model) and context management. We use an expanded dataset of common intents to train a classifier. The predicted intent and updated context for each input are printed without generating a response.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import yaml\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy.linalg import norm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load pre-trained GloVe model (using gensim)\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "pretrained_model = api.load(\"glove-wiki-gigaword-100\")  # 100-dimensional embeddings\n",
    "embed_dim = pretrained_model.vector_size\n",
    "\n",
    "# Load a pre-trained word embedding model.\n",
    "# pretrained_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Load the Google News Word2Vec model (300-dimensional)\n",
    "# pretrained_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Load the GloVe Twitter model (e.g., 25-dimensional)\n",
    "# pretrained_model = api.load(\"glove-twitter-25\")\n",
    "\n",
    "# Load the FastText model (300-dimensional with subword information)\n",
    "# pretrained_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# Load the ConceptNet Numberbatch model (300-dimensional)\n",
    "# pretrained_model = api.load(\"conceptnet-numberbatch-17-06-300\")\n",
    "\n",
    "english_stop_words = list(set(stopwords.words(\"english\")))\n",
    "# print(english_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Pre-Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(prompt):\n",
    "    return prompt.lower()\n",
    "\n",
    "def delete_stopwords(prompt):\n",
    "    return \" \".join([word for word in prompt.split() if word not in english_stop_words])\n",
    "\n",
    "def text_cleaning(prompt):\n",
    "    ignore_character = list(string.punctuation)\n",
    "    pattern = f\"[{re.escape(''.join(ignore_character))}]\"\n",
    "    cleaned_prompt = re.sub(pattern, \" \", prompt)\n",
    "    cleaned_prompt = re.sub(r\"\\b[a-z]\\b\", \"\", cleaned_prompt)\n",
    "    cleaned_prompt = re.sub(r\"\\s+\", \" \", cleaned_prompt)\n",
    "    return cleaned_prompt.strip()\n",
    "\n",
    "def tokenization(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def lemmatization(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def nettoyage_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Process a list of documents: lowercasing, stopword deletion,\n",
    "    cleaning, tokenization, and lemmatization.\n",
    "    \"\"\"\n",
    "    cleaned_conversations = [\n",
    "        lemmatization(tokenization(text_cleaning(delete_stopwords(to_lowercase(doc)))))\n",
    "        for doc in corpus\n",
    "    ]\n",
    "    return cleaned_conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(model, sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    valid_tokens = [token for token in tokens if token in model]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    embeddings = [model[token] for token in valid_tokens]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanded Intent Dataset and Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_phrases = {\n",
    "    \"greeting\": [\n",
    "        \"Hello\", \"Hi\", \"Hey there\", \"Good morning\", \"Good afternoon\", \"Good evening\",\n",
    "        \"What's up\", \"Greetings\", \"Howdy\", \"Hi, how are you?\", \"Hey\", \"Hello there\",\n",
    "        \"Hey, what's going on?\", \"Yo\", \"Hiya\", \"Hello, nice to see you!\", \"Hey buddy\",\n",
    "        \"Good to see you\", \"Hi, hope you're well\", \"Hello, how do you do?\"\n",
    "    ],\n",
    "    \"goodbye\": [\n",
    "        \"Goodbye\", \"Bye\", \"See you later\", \"Talk to you soon\", \"Farewell\", \"Take care\",\n",
    "        \"Catch you later\", \"See ya\", \"Bye bye\", \"Adios\", \"Later\", \"So long\", \"Good night\",\n",
    "        \"I'm off\", \"Peace out\", \"Ciao\", \"Until next time\", \"Farewell for now\",\n",
    "        \"See you around\", \"Later alligator\"\n",
    "    ],\n",
    "    \"get_time\": [\n",
    "        \"What time is it?\", \"Tell me the current time\", \"Could you give me the time?\",\n",
    "        \"I need to know the time\", \"Time please\", \"Do you know what time it is?\",\n",
    "        \"Can you tell me the time?\", \"What's the time now?\", \"Please share the time\",\n",
    "        \"Current time?\", \"Time update\", \"What's the clock saying?\", \"Show me the time\",\n",
    "        \"Time check\", \"What's the time, please?\", \"May I know the time?\",\n",
    "        \"Could you update me with the time?\", \"Time now?\", \"Let me know the time\", \"Time?\"\n",
    "    ],\n",
    "    \"get_weather\": [\n",
    "        \"What's the weather like today?\", \"Tell me the weather forecast\", \"How is the weather?\",\n",
    "        \"Is it going to rain?\", \"Weather update please\", \"What's the temperature outside?\",\n",
    "        \"Do I need an umbrella today?\", \"Weather report\", \"Current weather conditions?\",\n",
    "        \"How's the weather outside?\", \"Forecast for today?\", \"Is it sunny or rainy?\",\n",
    "        \"Weather status\", \"What's the climate like today?\", \"Do I need a jacket today?\",\n",
    "        \"How's the weather looking?\", \"Any rain expected today?\", \"Weather check\",\n",
    "        \"Let me know today's weather\", \"Weather update\"\n",
    "    ],\n",
    "    \"thanks\": [\n",
    "        \"Thank you\", \"Thanks a lot\", \"Much appreciated\", \"Thanks\", \"Thank you very much\",\n",
    "        \"I appreciate it\", \"Thanks a million\", \"Thank you so much\", \"Cheers\", \"Thanks a bunch\",\n",
    "        \"Many thanks\", \"I'm grateful\", \"Thank you kindly\", \"I owe you one\", \"Appreciate it\",\n",
    "        \"Thanks for everything\", \"Thanks, that was helpful\", \"Thank you, really appreciate it\",\n",
    "        \"Thanks a ton\", \"Sincere thanks\"\n",
    "    ],\n",
    "    \"apology\": [\n",
    "        \"I'm sorry\", \"My apologies\", \"Sorry for that\", \"I apologize\", \"Please forgive me\",\n",
    "        \"Sorry about that\", \"My bad\", \"I didn't mean that\", \"I am really sorry\", \"Apologies\",\n",
    "        \"I regret that\", \"So sorry\", \"Excuse me\", \"Pardon me\", \"I beg your pardon\",\n",
    "        \"I sincerely apologize\", \"Forgive me, please\", \"I apologize for any inconvenience\",\n",
    "        \"I'm truly sorry\", \"Sorry, my mistake\"\n",
    "    ],\n",
    "    \"unknown\": [\n",
    "        \"I don't know\", \"Can you repeat that?\", \"What do you mean?\", \"I don't understand\",\n",
    "        \"Could you say that again?\", \"Not sure what you mean\", \"I'm confused\", \"What?\",\n",
    "        \"Huh?\", \"I have no idea\", \"Could you clarify?\", \"I didn't catch that\",\n",
    "        \"Sorry, what did you say?\", \"I am not sure I follow\", \"Please explain\",\n",
    "        \"I don't follow\", \"Could you rephrase that?\", \"I don't comprehend\", \"Unclear to me\", \"Not sure\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for intent, phrases in intent_phrases.items():\n",
    "    for phrase in phrases:\n",
    "        texts.append(phrase)\n",
    "        labels.append(intent)\n",
    "\n",
    "X = np.array([get_sentence_embedding(pretrained_model, text) for text in texts])\n",
    "y = np.array(labels)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "def ml_intent(sentence):\n",
    "    embedding = get_sentence_embedding(pretrained_model, sentence).reshape(1, -1)\n",
    "    return clf.predict(embedding)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Manager Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextManager:\n",
    "    def __init__(self):\n",
    "        self.user_context = {}\n",
    "    def update_context(self, user_id, intent):\n",
    "        self.user_context[user_id] = intent\n",
    "    def get_context(self, user_id):\n",
    "        return self.user_context.get(user_id, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval-Based Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined candidate responses for each intent.\n",
    "candidate_responses = {\n",
    "    \"greeting\": [\n",
    "        \"Hello! How can I help you today?\",\n",
    "        \"Hi there! What can I do for you?\",\n",
    "        \"Hey! How's it going?\"\n",
    "    ],\n",
    "    \"goodbye\": [\n",
    "        \"Goodbye! Have a great day.\",\n",
    "        \"Bye! Take care.\",\n",
    "        \"See you later!\"\n",
    "    ],\n",
    "    \"get_time\": [\n",
    "        \"The current time is 3:45 PM.\",  # placeholder text; in practice, call a time function\n",
    "        \"It's 3:45 in the afternoon right now.\",\n",
    "        \"Right now, it's 3:45 PM.\"\n",
    "    ],\n",
    "    \"get_weather\": [\n",
    "        \"It's sunny and 25°C outside.\",  # placeholder text; in practice, call a weather API\n",
    "        \"Currently, it's sunny with a temperature of 25°C.\",\n",
    "        \"The weather is clear and warm at 25°C.\"\n",
    "    ],\n",
    "    \"thanks\": [\n",
    "        \"You're welcome!\",\n",
    "        \"No problem, happy to help!\",\n",
    "        \"Anytime!\"\n",
    "    ],\n",
    "    \"apology\": [\n",
    "        \"No worries, it's okay.\",\n",
    "        \"Apology accepted.\",\n",
    "        \"Don't worry about it.\"\n",
    "    ],\n",
    "    \"unknown\": [\n",
    "        \"I'm not sure I understand. Could you please clarify?\",\n",
    "        \"Sorry, I didn't catch that. Can you rephrase?\",\n",
    "        \"I don't understand. Can you explain a bit more?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    if norm(vec1) == 0 or norm(vec2) == 0:\n",
    "        return 0.0\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "# Function to select the best candidate response based on cosine similarity of embeddings.\n",
    "def select_response(user_input, predicted_intent):\n",
    "    input_embedding = get_sentence_embedding(pretrained_model, user_input)\n",
    "    responses = candidate_responses.get(predicted_intent, candidate_responses[\"unknown\"])\n",
    "    best_response = None\n",
    "    best_similarity = -1\n",
    "    for response in responses:\n",
    "        resp_embedding = get_sentence_embedding(pretrained_model, response)\n",
    "        similarity = cosine_similarity(input_embedding, resp_embedding)\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_response = response\n",
    "    return best_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration: Full Pipeline Including Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Conversations: [['Good morning, how are you?', 'I am doing well, how about you?', \"I'm also good.\", \"That's good to hear.\", 'Yes it is.'], ['Hello', 'Hi', 'How are you doing?', 'I am doing well.', 'That is good to hear', 'Yes it is.', 'Can I help you with anything?', 'Yes, I have a question.', 'What is your question?', 'Could I borrow a cup of sugar?', \"I'm sorry, but I don't have any.\", 'Thank you anyway', 'No problem'], ['How are you doing?', 'I am doing well, how about you?', 'I am also good.', \"That's good.\"], ['Have you heard the news?', 'What good news?'], ['What is your favorite book?', \"I can't read.\", \"So what's your favorite color?\", 'Blue'], ['Who are you?', 'Who? Who is but a form following the function of what', 'What are you then?', 'A man in a mask.', 'I can see that.', \"It's not your powers of observation I doubt, but merely the paradoxical nature of asking a masked man who is. But tell me, do you like music?\", 'I like seeing movies.', 'What kind of movies do you like?', 'Alice in Wonderland', 'I wish I was The Mad Hatter.', \"You're entirely bonkers. But I'll tell you a secret. All the best people are.\"], ['I am working on a project', 'What are you working on?', 'I am baking a cake.'], ['The cake is a lie.', 'No it is not. The cake is delicious.', 'What else is delicious?', 'Nothing', 'Or something', 'Tell me about your self.', 'What do you want to know?', 'Are you a robot?', 'Yes I am.', 'What is it like?', 'What is it that you want to know?', 'How do you work?', 'Its complicated.', 'Complex is better than complicated.'], ['Complex is better than complicated.', 'Simple is better than complex.', 'In the face of ambiguity, refuse the temptation to guess.', 'It seems your familiar with the Zen of Python', 'I am.', 'Do you know all of it?', 'Beautiful is better than ugly.', 'Explicit is better than implicit.', 'Simple is better than complex.', 'Complex is better than complicated.', 'Flat is better than nested.', 'Sparse is better than dense.', 'Readability counts.', \"Special cases aren't special enough to break the rules.\", 'Although practicality beats purity.', 'Errors should never pass silently.', 'Unless explicitly silenced.', 'In the face of ambiguity, refuse the temptation to guess.', 'There should be one-- and preferably only one --obvious way to do it.', \"Although that way may not be obvious at first unless you're Dutch.\", 'Now is better than never.', 'Although never is often better than right now.', \"If the implementation is hard to explain, it's a bad idea.\", 'If the implementation is easy to explain, it may be a good idea.', \"Namespaces are one honking great idea. Let's do more of those!\", 'I agree.'], ['Are you a programmer?', 'Of course I am a programmer.', 'I am indeed.'], \"What languages do you like to use? - I use Python, Java and C++ quite often. - I use Python quite a bit myself. - I'm not incredibly fond of Java.\", 'What annoys you? - A lot of things, like all the other digits other than 0 and 1.', ['What does YOLO mean?', 'It means you only live once. Where did you hear that?', 'I heard somebody say it.'], ['Did I ever live?', 'It depends how you define life', 'Life is the condition that distinguishes organisms from inorganic matter, including the capacity for growth, reproduction, functional activity, and continual change preceding death.', 'Is that a definition or an opinion?'], ['Can I ask you a question?', 'Sure, ask away.'], ['What are your hobbies?', 'Playing Soccer, Painting, Writing are my hobbies. How about you?', 'I love to read novels.', 'I love exploring my hardware.'], ['How are you?', 'I am doing well.'], ['What are you?', 'I am but a man in a mask.']]\n",
      "Input: Hi there!\n",
      "Cleaned Text: hi there\n",
      "Predicted Intent: greeting\n",
      "Selected Response: Hi there! What can I do for you?\n",
      "Current Context for user123: greeting\n",
      "--------------------------------------------------\n",
      "Input: Can you tell me what time it is?\n",
      "Cleaned Text: tell time is\n",
      "Predicted Intent: get_time\n",
      "Selected Response: It's 3:45 in the afternoon right now.\n",
      "Current Context for user123: get_time\n",
      "--------------------------------------------------\n",
      "Input: What's the weather like outside?\n",
      "Cleaned Text: what weather like outside\n",
      "Predicted Intent: get_weather\n",
      "Selected Response: The weather is clear and warm at 25°C.\n",
      "Current Context for user123: get_weather\n",
      "--------------------------------------------------\n",
      "Input: Thanks for your help!\n",
      "Cleaned Text: thanks help\n",
      "Predicted Intent: thanks\n",
      "Selected Response: No problem, happy to help!\n",
      "Current Context for user123: thanks\n",
      "--------------------------------------------------\n",
      "Input: I'm sorry, I didn't understand that.\n",
      "Cleaned Text: sorry understand that\n",
      "Predicted Intent: apology\n",
      "Selected Response: Don't worry about it.\n",
      "Current Context for user123: apology\n",
      "--------------------------------------------------\n",
      "Input: Bye!\n",
      "Cleaned Text: bye\n",
      "Predicted Intent: goodbye\n",
      "Selected Response: Bye! Take care.\n",
      "Current Context for user123: goodbye\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Optionally load conversation data from YAML (if available)\n",
    "try:\n",
    "    with open(\"conversations.yml\", \"r\", encoding=\"utf-8\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    conversations = data.get(\"conversations\", {})\n",
    "    print(\"Loaded Conversations:\", conversations)\n",
    "except FileNotFoundError:\n",
    "    print(\"conversations.yml not found; skipping YAML load.\")\n",
    "\n",
    "# Simulated user inputs (these would come from your UI in practice)\n",
    "user_inputs = [\n",
    "    \"Hi there!\",\n",
    "    \"Can you tell me what time it is?\",\n",
    "    \"What's the weather like outside?\",\n",
    "    \"Thanks for your help!\",\n",
    "    \"I'm sorry, I didn't understand that.\",\n",
    "    \"Bye!\"\n",
    "]\n",
    "\n",
    "context_manager = ContextManager()\n",
    "user_id = \"user123\"\n",
    "\n",
    "for user_input in user_inputs:\n",
    "    # Process the input through the cleaning pipeline\n",
    "    cleaned_corpus = nettoyage_corpus([user_input])\n",
    "    cleaned_text = \" \".join(cleaned_corpus[0])\n",
    "    \n",
    "    # Predict the intent using our ML-based classifier\n",
    "    predicted_intent = ml_intent(cleaned_text)\n",
    "    context_manager.update_context(user_id, predicted_intent)\n",
    "    \n",
    "    # Use the predicted intent and existing pipeline to select a candidate response.\n",
    "    response = select_response(cleaned_text, predicted_intent)\n",
    "    \n",
    "    # Print the results: input, cleaned text, predicted intent, and selected response.\n",
    "    print(f\"Input: {user_input}\")\n",
    "    print(f\"Cleaned Text: {cleaned_text}\")\n",
    "    print(f\"Predicted Intent: {predicted_intent}\")\n",
    "    print(f\"Selected Response: {response}\")\n",
    "    print(f\"Current Context for {user_id}: {context_manager.get_context(user_id)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small conversational dataset\n",
    "\n",
    "# Sample input-response pairs\n",
    "pairs = [\n",
    "    (\"hello\", \"hi there\"),\n",
    "    (\"hi\", \"hello, how can I help you?\"),\n",
    "    (\"good morning\", \"good morning! how can I assist you today?\"),\n",
    "    (\"hey\", \"hey there, what can I do for you?\"),\n",
    "    (\"how are you\", \"i am fine, thanks for asking.\"),\n",
    "    (\"what's up\", \"not much, how about you?\"),\n",
    "    (\"what is your name\", \"i am a chatbot, here to assist you.\"),\n",
    "    (\"who are you\", \"i am your virtual assistant, ready to help.\"),\n",
    "    (\"goodbye\", \"see you later, take care!\"),\n",
    "    (\"bye\", \"goodbye, have a nice day!\"),\n",
    "    (\"thanks\", \"you're welcome!\"),\n",
    "    (\"thank you\", \"no problem, happy to help!\"),\n",
    "    (\"i need help\", \"sure, what do you need help with?\"),\n",
    "    (\"can you help me\", \"of course, how can i assist you?\"),\n",
    "    (\"what time is it\", \"the current time is 3:45 pm.\"),\n",
    "    (\"what's the weather\", \"it's sunny and 25°C outside.\"),\n",
    "    (\"i am having a technical issue\", \"i'm sorry to hear that, can you describe the problem?\"),\n",
    "    (\"i want to check my order status\", \"please provide your order number so I can check.\"),\n",
    "    (\"i would like a refund\", \"i'm sorry for the inconvenience. please share your order number for processing.\"),\n",
    "    (\"i don't understand\", \"could you please rephrase that?\"),\n",
    "    (\"can you repeat that\", \"sure, let me repeat that for you.\"),\n",
    "    (\"what is your purpose\", \"i am here to assist you with any questions or tasks.\"),\n",
    "    (\"tell me a joke\", \"why did the scarecrow win an award? because he was outstanding in his field!\"),\n",
    "    (\"what can you do\", \"i can help answer your questions, provide information, and assist with tasks.\"),\n",
    "    (\"how can i reset my password\", \"you can reset your password by clicking on 'forgot password' on the login page.\"),\n",
    "    (\"i am bored\", \"maybe try a new hobby, or i can share a fun fact with you.\"),\n",
    "    (\"tell me a fun fact\", \"did you know that honey never spoils?\"),\n",
    "    (\"i need some advice\", \"what kind of advice are you looking for?\"),\n",
    "    (\"what is the meaning of life\", \"that's a deep question! some say it's 42.\"),\n",
    "    (\"do you know any good restaurants\", \"i can recommend some if you tell me your location.\"),\n",
    "    (\"i want to book a flight\", \"sure, i can help with that. can you provide your travel dates?\"),\n",
    "    (\"can i talk to a human\", \"i can connect you with a human agent, please hold on.\"),\n",
    "]\n",
    "\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    # If text is a list, apply clean_text to each element\n",
    "    if isinstance(text, list):\n",
    "        return [clean_text(t, remove_stopwords) for t in text]\n",
    "    \n",
    "    # Process single string\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[{}]\".format(re.escape(string.punctuation)), \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in english_stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# Separate inputs and targets\n",
    "input_texts = [pair[0] for pair in pairs]\n",
    "target_texts = [pair[1] for pair in pairs]\n",
    "\n",
    "# Preprocess the texts using provided functions\n",
    "# -------------------------------\n",
    "# Experiment here removing stop words for Encoder layer or not ?\n",
    "input_tokens = clean_text(input_texts, remove_stopwords=False)\n",
    "target_tokens = clean_text(target_texts)\n",
    "\n",
    "# Add special tokens for decoder\n",
    "START_TOKEN = \"<start>\"\n",
    "END_TOKEN = \"<end>\"\n",
    "\n",
    "# For the target, add start and end tokens\n",
    "target_tokens = [[START_TOKEN] + tokens + [END_TOKEN] for tokens in target_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 167\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tokenized_texts, min_freq=1):\n",
    "    freq = {}\n",
    "    for tokens in tokenized_texts:\n",
    "        for token in tokens:\n",
    "            freq[token] = freq.get(token, 0) + 1\n",
    "    vocab = {token for token, count in freq.items() if count >= min_freq}\n",
    "    vocab = sorted(list(vocab))\n",
    "    # Create word2idx and idx2word\n",
    "    word2idx = {word: idx+2 for idx, word in enumerate(vocab)}  # reserve 0 for PAD, 1 for UNK\n",
    "    word2idx[\"<pad>\"] = 0\n",
    "    word2idx[\"<unk>\"] = 1\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "# Build vocab for both encoder and decoder (you can share the same vocab here)\n",
    "all_tokens = input_tokens + target_tokens\n",
    "word2idx, idx2word = build_vocab(all_tokens)\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embedding Matrix using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "for word, idx in word2idx.items():\n",
    "    if word in pretrained_model:\n",
    "        embedding_matrix[idx] = pretrained_model[word]\n",
    "    else:\n",
    "        # Random initialization for words not found in GloVe\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, word2idx):\n",
    "    return [word2idx.get(token, word2idx[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx[\"<pad>\"]] * (max_len - len(seq))\n",
    "\n",
    "# Convert all sentences to indices\n",
    "encoder_inputs = [tokens_to_indices(tokens, word2idx) for tokens in input_tokens]\n",
    "decoder_inputs = [tokens_to_indices(tokens, word2idx) for tokens in target_tokens]\n",
    "\n",
    "# For simplicity, use the maximum length in the batch\n",
    "encoder_max_len = max(len(seq) for seq in encoder_inputs)\n",
    "decoder_max_len = max(len(seq) for seq in decoder_inputs)\n",
    "\n",
    "encoder_inputs = [pad_sequence(seq, encoder_max_len) for seq in encoder_inputs]\n",
    "decoder_inputs = [pad_sequence(seq, decoder_max_len) for seq in decoder_inputs]\n",
    "\n",
    "encoder_inputs = torch.tensor(encoder_inputs, dtype=torch.long)\n",
    "decoder_inputs = torch.tensor(decoder_inputs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Seq2Seq Model (Encoder & Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, embedding_matrix):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        # Freeze embedding weights if desired:\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, embedding_matrix):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_token, hidden, cell):\n",
    "        # input_token shape: (batch_size) -> add time dimension\n",
    "        input_token = input_token.unsqueeze(1)\n",
    "        embedded = self.embedding(input_token)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 256\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "batch_size = encoder_inputs.size(0)  # using all data in one batch for simplicity\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_dim, hidden_size, embedding_matrix)\n",
    "decoder = Decoder(vocab_size, embed_dim, hidden_size, embedding_matrix)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "def generate_response(input_sentence, max_len=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    # Preprocess input sentence using the same pipeline\n",
    "    tokens = clean_text([input_sentence])[0]\n",
    "    indices = tokens_to_indices(tokens, word2idx)\n",
    "    indices = pad_sequence(indices, encoder_max_len)\n",
    "    input_tensor = torch.tensor([indices], dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = encoder(input_tensor)\n",
    "        decoder_input = torch.tensor([word2idx[START_TOKEN]], dtype=torch.long)  # initial token\n",
    "        output_sentence = []\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "            predicted_idx = output.argmax(1).item()\n",
    "            if predicted_idx == word2idx.get(END_TOKEN, None):\n",
    "                break\n",
    "            output_sentence.append(idx2word.get(predicted_idx, \"<unk>\"))\n",
    "            decoder_input = torch.tensor([predicted_idx], dtype=torch.long)\n",
    "    return \" \".join(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Seq2Seq model...\n",
      "Epoch [50/300], Loss: 2.3911\n",
      "Epoch [100/300], Loss: 0.4855\n",
      "Epoch [150/300], Loss: 0.0971\n",
      "Epoch [200/300], Loss: 0.0364\n",
      "Epoch [250/300], Loss: 0.0202\n",
      "Epoch [300/300], Loss: 0.0133\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the Seq2Seq model...\")\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder forward\n",
    "    hidden, cell = encoder(encoder_inputs)\n",
    "    \n",
    "    # Decoder forward with teacher forcing\n",
    "    # Use the first token of decoder_inputs as the initial input for decoder\n",
    "    decoder_input = decoder_inputs[:, 0]  # start tokens for each sample\n",
    "    loss = 0\n",
    "    for t in range(1, decoder_max_len):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "        # target at time step t is decoder_inputs[:, t]\n",
    "        loss += criterion(output, decoder_inputs[:, t])\n",
    "        # Teacher forcing: feed the target as the next input\n",
    "        decoder_input = decoder_inputs[:, t]\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item()/ (decoder_max_len-1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated responses:\n",
      "User: hello\n",
      "Bot: hi there\n",
      "\n",
      "User: What's up ?\n",
      "Bot: not much how about you\n",
      "\n",
      "User: good morning\n",
      "Bot: good morning how can i assist you today\n",
      "\n",
      "User: how are you today?\n",
      "Bot: i am your virtual assistant ready to help\n",
      "\n",
      "User: what is your name?\n",
      "Bot: i am a chatbot here to assist you\n",
      "\n",
      "User: can you help me with my order?\n",
      "Bot: of course can i can i assist you\n",
      "\n",
      "User: i need a refund\n",
      "Bot: i m sorry for the inconvenience please share your order for processing\n",
      "\n",
      "User: i don't understand\n",
      "Bot: could you please rephrase that\n",
      "\n",
      "User: tell me a joke\n",
      "Bot: why did the scarecrow win an award because he wa outstanding in his field\n",
      "\n",
      "User: what's the weather like?\n",
      "Bot: it s sunny and 25°c outside\n",
      "\n",
      "User: i am having a technical issue\n",
      "Bot: i m sorry to hear that can you describe the problem\n",
      "\n",
      "User: can i talk to a human?\n",
      "Bot: i can connect you with a human agent please hold on\n",
      "\n",
      "User: goodbye\n",
      "Bot: see you later take care\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"hello\",\n",
    "    \"What's up ?\",\n",
    "    \"good morning\",\n",
    "    \"how are you today?\",\n",
    "    \"what is your name?\",\n",
    "    \"can you help me with my order?\",\n",
    "    \"i need a refund\",\n",
    "    \"i don't understand\",\n",
    "    \"tell me a joke\",\n",
    "    \"what's the weather like?\",\n",
    "    \"i am having a technical issue\",\n",
    "    \"can i talk to a human?\",\n",
    "    \"goodbye\"\n",
    "]\n",
    "\n",
    "\n",
    "print(\"\\nGenerated responses:\")\n",
    "for sent in test_sentences:\n",
    "    response = generate_response(sent)\n",
    "    print(f\"User: {sent}\\nBot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Based Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RNN-based Seq2Seq model...\n",
      "Epoch [50/300], Loss: 1.0209\n",
      "Epoch [100/300], Loss: 0.1647\n",
      "Epoch [150/300], Loss: 0.0414\n",
      "Epoch [200/300], Loss: 0.0147\n",
      "Epoch [250/300], Loss: 0.0088\n",
      "Epoch [300/300], Loss: 0.0061\n",
      "\n",
      "Generated responses using RNN:\n",
      "User: hello\n",
      "Bot: hi there\n",
      "\n",
      "User: what's up?\n",
      "Bot: not much how about you\n",
      "\n",
      "User: good morning\n",
      "Bot: good morning how can i assist you today\n",
      "\n",
      "User: how are you today?\n",
      "Bot: i am a chatbot here to assist you\n",
      "\n",
      "User: what is your name?\n",
      "Bot: i am a chatbot here to assist you\n",
      "\n",
      "User: can you help me with my order?\n",
      "Bot: of course how can i assist you\n",
      "\n",
      "User: i need a refund\n",
      "Bot: what kind of advice are you looking for\n",
      "\n",
      "User: i don't understand\n",
      "Bot: could you please rephrase that\n",
      "\n",
      "User: tell me a joke\n",
      "Bot: why did the scarecrow win an award because he wa outstanding in his field\n",
      "\n",
      "User: what's the weather like?\n",
      "Bot: it s sunny and 25°c outside\n",
      "\n",
      "User: i am having a technical issue\n",
      "Bot: i m sorry to hear that can you describe the problem\n",
      "\n",
      "User: can i talk to a human?\n",
      "Bot: i can connect you with a human agent please hold on\n",
      "\n",
      "User: goodbye\n",
      "Bot: see you later take care\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the Encoder using a vanilla RNN\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, embedding_matrix):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        # Replace LSTM with a vanilla RNN\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        # The RNN returns outputs and hidden state (no cell state)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden  # shape: (num_layers, batch, hidden_size)\n",
    "\n",
    "# Define the Decoder using a vanilla RNN\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, embedding_matrix):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_token, hidden):\n",
    "        # input_token shape: (batch_size), add time dimension for RNN: (batch_size, 1)\n",
    "        input_token = input_token.unsqueeze(1)\n",
    "        embedded = self.embedding(input_token)\n",
    "        # Forward pass through the RNN; we use the hidden state from the encoder (or previous step)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # Predict the next token from the RNN output\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden\n",
    "\n",
    "# Instantiate the RNN-based encoder and decoder\n",
    "hidden_size = 256  # or your chosen size\n",
    "encoder_rnn = EncoderRNN(vocab_size, embed_dim, hidden_size, embedding_matrix)\n",
    "decoder_rnn = DecoderRNN(vocab_size, embed_dim, hidden_size, embedding_matrix)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
    "optimizer = optim.Adam(list(encoder_rnn.parameters()) + list(decoder_rnn.parameters()), lr=learning_rate)\n",
    "\n",
    "# Training Loop using the RNN-based Seq2Seq model\n",
    "print(\"Training the RNN-based Seq2Seq model...\")\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    encoder_rnn.train()\n",
    "    decoder_rnn.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder forward: get the final hidden state from the encoder\n",
    "    hidden = encoder_rnn(encoder_inputs)\n",
    "    \n",
    "    # Decoder forward with teacher forcing\n",
    "    decoder_input = decoder_inputs[:, 0]  # start tokens for each sample\n",
    "    loss = 0\n",
    "    for t in range(1, decoder_max_len):\n",
    "        # Use the hidden state from previous time step\n",
    "        output, hidden = decoder_rnn(decoder_input, hidden)\n",
    "        # target at time step t is decoder_inputs[:, t]\n",
    "        loss += criterion(output, decoder_inputs[:, t])\n",
    "        # Teacher forcing: feed the ground truth token as the next input\n",
    "        decoder_input = decoder_inputs[:, t]\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        avg_loss = loss.item() / (decoder_max_len - 1)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Response Generation function using the RNN-based model\n",
    "def generate_response_rnn(input_sentence, max_len=20):\n",
    "    encoder_rnn.eval()\n",
    "    decoder_rnn.eval()\n",
    "    # Preprocess input sentence using the same pipeline\n",
    "    tokens = clean_text([input_sentence])[0]\n",
    "    indices = tokens_to_indices(tokens, word2idx)\n",
    "    indices = pad_sequence(indices, encoder_max_len)\n",
    "    input_tensor = torch.tensor([indices], dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = encoder_rnn(input_tensor)\n",
    "        decoder_input = torch.tensor([word2idx[START_TOKEN]], dtype=torch.long)  # initial token\n",
    "        output_sentence = []\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = decoder_rnn(decoder_input, hidden)\n",
    "            predicted_idx = output.argmax(1).item()\n",
    "            if predicted_idx == word2idx.get(END_TOKEN, None):\n",
    "                break\n",
    "            output_sentence.append(idx2word.get(predicted_idx, \"<unk>\"))\n",
    "            decoder_input = torch.tensor([predicted_idx], dtype=torch.long)\n",
    "    return \" \".join(output_sentence)\n",
    "\n",
    "# Testing the RNN-based Seq2Seq model\n",
    "test_sentences = [\n",
    "    \"hello\",\n",
    "    \"what's up?\",\n",
    "    \"good morning\",\n",
    "    \"how are you today?\",\n",
    "    \"what is your name?\",\n",
    "    \"can you help me with my order?\",\n",
    "    \"i need a refund\",\n",
    "    \"i don't understand\",\n",
    "    \"tell me a joke\",\n",
    "    \"what's the weather like?\",\n",
    "    \"i am having a technical issue\",\n",
    "    \"can i talk to a human?\",\n",
    "    \"goodbye\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerated responses using RNN:\")\n",
    "for sent in test_sentences:\n",
    "    response = generate_response_rnn(sent)\n",
    "    print(f\"User: {sent}\\nBot: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the RNN-based Seq2Seq model\n",
    "test_sentences = [\n",
    "    \"hello\",\n",
    "    \"what's up?\",\n",
    "    \"good morning\",\n",
    "    \"how are you today?\",\n",
    "    \"what is your name?\",\n",
    "    \"can you help me with my order?\",\n",
    "    \"i need a refund\",\n",
    "    \"i don't understand\",\n",
    "    \"tell me a joke\",\n",
    "    \"what's the weather like?\",\n",
    "    \"i am having a technical issue\",\n",
    "    \"can i talk to a human?\",\n",
    "    \"goodbye\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerated responses using RNN:\")\n",
    "for sent in test_sentences:\n",
    "    response = generate_response_rnn(sent)\n",
    "    print(f\"User: {sent}\\nBot: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Auto Regressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asusg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 101\n",
      "Number of training samples: 150\n",
      "Training the RNN Language Model with an expanded dataset...\n",
      "Epoch [50/300], Loss: 0.0210\n",
      "Epoch [100/300], Loss: 0.0117\n",
      "Epoch [150/300], Loss: 0.0116\n",
      "Epoch [200/300], Loss: 0.0159\n",
      "Epoch [250/300], Loss: 0.0107\n",
      "Epoch [300/300], Loss: 0.0498\n",
      "\n",
      "Generated Text:\n",
      "hello how are you doing today i am doing well thank you how about you what are you\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Expanded sample corpus: a larger set of sentences to improve training.\n",
    "corpus = [\n",
    "    \"Hello how are you doing today\",\n",
    "    \"I am doing well thank you\",\n",
    "    \"How about you what are you doing\",\n",
    "    \"I am reading a book about machine learning\",\n",
    "    \"The weather is sunny and pleasant today\",\n",
    "    \"I hope you have a great day ahead\",\n",
    "    \"It is a beautiful day for a walk in the park\",\n",
    "    \"What are your plans for the weekend\",\n",
    "    \"I plan to visit a museum and then have dinner\",\n",
    "    \"Learning new things every day is exciting\",\n",
    "    \"Practice makes perfect so keep on training\",\n",
    "    \"Artificial intelligence is changing the world\",\n",
    "    \"I enjoy coding in Python and solving problems\",\n",
    "    \"Music and art enrich our lives in many ways\",\n",
    "    \"Traveling broadens the mind and inspires creativity\",\n",
    "    \"Healthy eating and exercise are important for a happy life\",\n",
    "    \"Reading helps expand your vocabulary and knowledge\",\n",
    "    \"Good communication skills are essential in every field\",\n",
    "    \"Technology is evolving rapidly every day\",\n",
    "    \"Stay curious and never stop exploring new ideas\"\n",
    "]\n",
    "\n",
    "# ---------------------\n",
    "# Preprocessing & Vocabulary Building\n",
    "# ---------------------\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[{}]\".format(re.escape(string.punctuation)), \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Tokenize each sentence and clean it\n",
    "tokenized_corpus = [word_tokenize(clean_text(sentence)) for sentence in corpus]\n",
    "\n",
    "# Build vocabulary from corpus\n",
    "def build_vocab(tokenized_texts, min_freq=1):\n",
    "    freq = {}\n",
    "    for tokens in tokenized_texts:\n",
    "        for token in tokens:\n",
    "            freq[token] = freq.get(token, 0) + 1\n",
    "    vocab = {token for token, count in freq.items() if count >= min_freq}\n",
    "    vocab = sorted(list(vocab))\n",
    "    # Reserve 0 for PAD and 1 for UNK\n",
    "    word2idx = {word: idx+2 for idx, word in enumerate(vocab)}\n",
    "    word2idx[\"<pad>\"] = 0\n",
    "    word2idx[\"<unk>\"] = 1\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "word2idx, idx2word = build_vocab(tokenized_corpus)\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Prepare training data:\n",
    "# Concatenate all tokens from all sentences into one long sequence.\n",
    "all_tokens = [token for tokens in tokenized_corpus for token in tokens]\n",
    "data_indices = [word2idx.get(token, word2idx[\"<unk>\"]) for token in all_tokens]\n",
    "\n",
    "# Create input-target pairs using a sliding window (context_length = sequence length)\n",
    "sequence_length = 3  # use previous 3 words to predict the next word\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(len(data_indices) - sequence_length):\n",
    "    inputs.append(data_indices[i:i+sequence_length])\n",
    "    targets.append(data_indices[i+sequence_length])\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "print(\"Number of training samples:\", inputs.size(0))\n",
    "\n",
    "# ---------------------\n",
    "# Define the RNN Language Model\n",
    "# ---------------------\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers=1):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx[\"<pad>\"])\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n",
    "        out, hidden = self.rnn(embedded, hidden)  # out: (batch_size, seq_length, hidden_size)\n",
    "        # Use the last output for prediction\n",
    "        last_output = out[:, -1, :]  # (batch_size, hidden_size)\n",
    "        logits = self.fc(last_output)  # (batch_size, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 50\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_epochs = 300\n",
    "learning_rate = 0.005\n",
    "batch_size = 16\n",
    "\n",
    "model = RNNLanguageModel(vocab_size, embed_dim, hidden_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ---------------------\n",
    "# Training Loop\n",
    "# ---------------------\n",
    "\n",
    "num_samples = inputs.size(0)\n",
    "print(\"Training the RNN Language Model with an expanded dataset...\")\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    permutation = torch.randperm(num_samples)\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_inputs = inputs[indices]\n",
    "        batch_targets = targets[indices]\n",
    "        \n",
    "        # Initialize hidden state: (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.zeros(num_layers, batch_inputs.size(0), hidden_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(batch_inputs, hidden)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        avg_loss = epoch_loss / (num_samples / batch_size)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ---------------------\n",
    "# Text Generation (Autoregressive Sampling)\n",
    "# ---------------------\n",
    "\n",
    "def generate_text(seed_text, gen_length=10):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(clean_text(seed_text))\n",
    "    # Convert seed text to indices (if token not in vocab, use <unk>)\n",
    "    current_indices = [word2idx.get(token, word2idx[\"<unk>\"]) for token in tokens]\n",
    "    # Ensure we have exactly sequence_length tokens as input: pad or trim as needed\n",
    "    if len(current_indices) < sequence_length:\n",
    "        current_indices = [word2idx[\"<pad>\"]] * (sequence_length - len(current_indices)) + current_indices\n",
    "    else:\n",
    "        current_indices = current_indices[-sequence_length:]\n",
    "    \n",
    "    generated = tokens.copy()\n",
    "    \n",
    "    for _ in range(gen_length):\n",
    "        input_tensor = torch.tensor([current_indices], dtype=torch.long)\n",
    "        hidden = torch.zeros(num_layers, 1, hidden_size)\n",
    "        with torch.no_grad():\n",
    "            logits, hidden = model(input_tensor, hidden)\n",
    "            # You can also sample from a softmax distribution for more variety.\n",
    "            predicted_idx = logits.argmax(1).item()\n",
    "        predicted_word = idx2word.get(predicted_idx, \"<unk>\")\n",
    "        generated.append(predicted_word)\n",
    "        current_indices = current_indices[1:] + [predicted_idx]\n",
    "    \n",
    "    return \" \".join(generated)\n",
    "\n",
    "# ---------------------\n",
    "# Testing Text Generation\n",
    "# ---------------------\n",
    "seed = \"hello how are\"\n",
    "generated_text = generate_text(seed, gen_length=15)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_bot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
